"""
í…ìŠ¤íŠ¸ ì²­í‚¹ ëª¨ë“ˆ (STEP 2)
- ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• 
- ë¬¸ë‹¨/ë¬¸ì¥/í† í° ê¸°ë°˜ ì²­í‚¹ ì§€ì›
- output: data/chunks/ ì•„ë˜ json íŒŒì¼ ì €ì¥

ì²­í‚¹ ê¸°ì¤€:
- chunk_size: 500ì (í•œêµ­ì–´ ê¸°ì¤€ ì ì ˆí•œ í¬ê¸°)
- overlap: 100ì (ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•œ ì˜¤ë²„ë©)
- ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìš°ì„  ë¶„í•  í›„, ê¸´ ë¬¸ë‹¨ì€ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¬ë¶„í• 
"""

import os
import json
import re
from pathlib import Path
from typing import List, Dict, Optional
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
TEXT_DIR = DATA_DIR / "text"
CHUNKS_DIR = DATA_DIR / "chunks"

# ì²­í‚¹ ì„¤ì •
DEFAULT_CHUNK_SIZE = 500  # ì²­í¬ë‹¹ ìµœëŒ€ ê¸€ì ìˆ˜
DEFAULT_OVERLAP = 100     # ì²­í¬ ê°„ ì˜¤ë²„ë© ê¸€ì ìˆ˜


def load_text_from_json(json_path: str) -> List[Dict]:
    """
    JSON íŒŒì¼ì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get('pages', [])


def load_text_from_txt(txt_path: str) -> str:
    """
    TXT íŒŒì¼ì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    with open(txt_path, 'r', encoding='utf-8') as f:
        return f.read()


def clean_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
    """
    if not text:
        return ""
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'[ \t]+', ' ', text)
    # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ìµœëŒ€ 2ê°œë¡œ
    text = re.sub(r'\n{3,}', '\n\n', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    return text


def split_into_paragraphs(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    """
    # ë¹ˆ ì¤„ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ ë¶„í• 
    paragraphs = re.split(r'\n\s*\n', text)
    # ë¹ˆ ë¬¸ë‹¨ ì œê±° ë° ì •ë¦¬
    paragraphs = [p.strip() for p in paragraphs if p.strip()]
    return paragraphs


def split_into_sentences(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸ ê¸°ì¤€
    """
    # ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸ë¡œ ë¶„í•  (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ)
    sentences = re.split(r'(?<=[.?!ã€‚])\s+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    return sentences


def create_chunks_with_overlap(
    text: str,
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    overlap: int = DEFAULT_OVERLAP
) -> List[Dict]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì˜¤ë²„ë©ì´ ìˆëŠ” ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    
    Args:
        text: ë¶„í• í•  í…ìŠ¤íŠ¸
        chunk_size: ì²­í¬ë‹¹ ìµœëŒ€ ê¸€ì ìˆ˜
        overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© ê¸€ì ìˆ˜
        
    Returns:
        ì²­í¬ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    chunks = []
    text = clean_text(text)
    
    if not text:
        return chunks
    
    # ë¬¸ë‹¨ìœ¼ë¡œ ë¨¼ì € ë¶„í• 
    paragraphs = split_into_paragraphs(text)
    
    current_chunk = ""
    chunk_id = 0
    
    for para in paragraphs:
        # ë¬¸ë‹¨ì´ chunk_sizeë³´ë‹¤ í¬ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        if len(para) > chunk_size:
            sentences = split_into_sentences(para)
            for sent in sentences:
                if len(current_chunk) + len(sent) + 1 <= chunk_size:
                    current_chunk += (" " if current_chunk else "") + sent
                else:
                    if current_chunk:
                        chunks.append({
                            "chunk_id": chunk_id,
                            "text": current_chunk.strip(),
                            "char_count": len(current_chunk.strip())
                        })
                        chunk_id += 1
                        # ì˜¤ë²„ë© ì ìš©
                        if overlap > 0 and len(current_chunk) > overlap:
                            current_chunk = current_chunk[-overlap:] + " " + sent
                        else:
                            current_chunk = sent
                    else:
                        # ë¬¸ì¥ ìì²´ê°€ ë„ˆë¬´ ê¸¸ë©´ ê°•ì œ ë¶„í• 
                        if len(sent) > chunk_size:
                            for i in range(0, len(sent), chunk_size - overlap):
                                chunk_text = sent[i:i + chunk_size]
                                chunks.append({
                                    "chunk_id": chunk_id,
                                    "text": chunk_text.strip(),
                                    "char_count": len(chunk_text.strip())
                                })
                                chunk_id += 1
                        else:
                            current_chunk = sent
        else:
            # ë¬¸ë‹¨ì„ í˜„ì¬ ì²­í¬ì— ì¶”ê°€
            if len(current_chunk) + len(para) + 2 <= chunk_size:
                current_chunk += ("\n\n" if current_chunk else "") + para
            else:
                if current_chunk:
                    chunks.append({
                        "chunk_id": chunk_id,
                        "text": current_chunk.strip(),
                        "char_count": len(current_chunk.strip())
                    })
                    chunk_id += 1
                    # ì˜¤ë²„ë© ì ìš©
                    if overlap > 0 and len(current_chunk) > overlap:
                        current_chunk = current_chunk[-overlap:] + "\n\n" + para
                    else:
                        current_chunk = para
                else:
                    current_chunk = para
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk:
        chunks.append({
            "chunk_id": chunk_id,
            "text": current_chunk.strip(),
            "char_count": len(current_chunk.strip())
        })
    
    return chunks


def chunk_by_pages(
    pages_data: List[Dict],
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    overlap: int = DEFAULT_OVERLAP
) -> List[Dict]:
    """
    í˜ì´ì§€ë³„ ë°ì´í„°ë¥¼ ì²­í‚¹í•˜ê³  í˜ì´ì§€ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ í¬í•¨í•©ë‹ˆë‹¤.
    """
    all_chunks = []
    global_chunk_id = 0
    
    for page in pages_data:
        page_num = page.get('page_number', 0)
        text = page.get('text', '')
        
        if not text:
            continue
        
        page_chunks = create_chunks_with_overlap(text, chunk_size, overlap)
        
        for chunk in page_chunks:
            chunk['global_chunk_id'] = global_chunk_id
            chunk['page_number'] = page_num
            chunk['source_type'] = 'page'
            all_chunks.append(chunk)
            global_chunk_id += 1
    
    return all_chunks


def save_chunks_to_json(chunks: List[Dict], output_path: str, metadata: Dict = None) -> None:
    """
    ì²­í¬ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    output_data = {
        "metadata": metadata or {},
        "total_chunks": len(chunks),
        "total_chars": sum(c['char_count'] for c in chunks),
        "avg_chunk_size": sum(c['char_count'] for c in chunks) / len(chunks) if chunks else 0,
        "chunks": chunks
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ì²­í¬ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")


def process_text_file(
    input_path: str,
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    overlap: int = DEFAULT_OVERLAP
) -> None:
    """
    ë‹¨ì¼ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì²­í‚¹í•©ë‹ˆë‹¤.
    """
    input_file = Path(input_path)
    
    if not input_file.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {input_file.name}")
    print(f"{'='*60}")
    
    # JSON íŒŒì¼ì´ë©´ í˜ì´ì§€ë³„ ì²˜ë¦¬
    if input_file.suffix == '.json':
        pages_data = load_text_from_json(str(input_file))
        chunks = chunk_by_pages(pages_data, chunk_size, overlap)
        print(f"ğŸ“– {len(pages_data)} í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ë¡œë“œ")
    else:
        # TXT íŒŒì¼ì´ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        text = load_text_from_txt(str(input_file))
        chunks = create_chunks_with_overlap(text, chunk_size, overlap)
        print(f"ğŸ“– í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
    
    print(f"âœ‚ï¸  ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    CHUNKS_DIR.mkdir(parents=True, exist_ok=True)
    
    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    base_name = input_file.stem.replace('.json', '').replace('.txt', '')
    output_path = CHUNKS_DIR / f"{base_name}_chunks.json"
    
    # ë©”íƒ€ë°ì´í„° ìƒì„±
    metadata = {
        "source_file": input_file.name,
        "chunk_size": chunk_size,
        "overlap": overlap,
        "chunking_method": "paragraph_based_with_sentence_fallback"
    }
    
    # ì €ì¥
    save_chunks_to_json(chunks, str(output_path), metadata)
    
    # í†µê³„ ì¶œë ¥
    if chunks:
        avg_size = sum(c['char_count'] for c in chunks) / len(chunks)
        min_size = min(c['char_count'] for c in chunks)
        max_size = max(c['char_count'] for c in chunks)
        print(f"\nğŸ“Š ì²­í‚¹ í†µê³„:")
        print(f"   - ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
        print(f"   - í‰ê·  í¬ê¸°: {avg_size:.1f}ì")
        print(f"   - ìµœì†Œ í¬ê¸°: {min_size}ì")
        print(f"   - ìµœëŒ€ í¬ê¸°: {max_size}ì")


def process_all_text_files(
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    overlap: int = DEFAULT_OVERLAP
) -> None:
    """
    data/text/ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  JSON íŒŒì¼ì„ ì²­í‚¹í•©ë‹ˆë‹¤.
    """
    json_files = list(TEXT_DIR.glob("*.json"))
    
    if not json_files:
        print("âš ï¸ ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ë¨¼ì € pdf_extracting.pyë¥¼ ì‹¤í–‰í•˜ì—¬ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.")
        return
    
    print(f"\nğŸ” ë°œê²¬ëœ í…ìŠ¤íŠ¸ íŒŒì¼: {len(json_files)}ê°œ")
    for f in json_files:
        print(f"   - {f.name}")
    
    for json_file in json_files:
        process_text_file(str(json_file), chunk_size, overlap)
    
    print(f"\nâœ… ëª¨ë“  íŒŒì¼ ì²­í‚¹ ì™„ë£Œ!")


if __name__ == "__main__":
    print("\n" + "="*60)
    print("âœ‚ï¸  í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œì‘")
    print(f"   - ì²­í¬ í¬ê¸°: {DEFAULT_CHUNK_SIZE}ì")
    print(f"   - ì˜¤ë²„ë©: {DEFAULT_OVERLAP}ì")
    print("="*60)
    
    if len(sys.argv) > 1:
        # íŠ¹ì • íŒŒì¼ ì²˜ë¦¬
        input_path = sys.argv[1]
        chunk_size = int(sys.argv[2]) if len(sys.argv) > 2 else DEFAULT_CHUNK_SIZE
        overlap = int(sys.argv[3]) if len(sys.argv) > 3 else DEFAULT_OVERLAP
        process_text_file(input_path, chunk_size, overlap)
    else:
        # ëª¨ë“  í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
        process_all_text_files()
    
    print("\n" + "="*60)
    print("ğŸ‰ ì²­í‚¹ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {CHUNKS_DIR}")
    print("="*60 + "\n")


