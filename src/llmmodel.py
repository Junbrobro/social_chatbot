"""
LLM API í˜¸ì¶œ ëª¨ë“ˆ (STEP 6-1)
- ë¬´ë£Œ LLM ì‚¬ìš© (Ollama, HuggingFace, OpenAI í˜¸í™˜ API ë“±)
- ë‹¤ì–‘í•œ LLM ë°±ì—”ë“œ ì§€ì›
"""

import os
from typing import List, Dict, Optional, Generator
from abc import ABC, abstractmethod


class BaseLLM(ABC):
    """LLM ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        pass
    
    @abstractmethod
    def chat(self, messages: List[Dict], **kwargs) -> str:
        """ì±„íŒ… í˜•ì‹ ìƒì„±"""
        pass


class OllamaLLM(BaseLLM):
    """
    Ollama ë¡œì»¬ LLM
    ì„¤ì¹˜: https://ollama.ai
    """
    
    def __init__(
        self,
        model: str = "gemma3:4b",
        base_url: str = "http://localhost:11434"
    ):
        self.model = model
        self.base_url = base_url
    
    def generate(self, prompt: str, **kwargs) -> str:
        import requests
        
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                **kwargs
            }
        )
        response.raise_for_status()
        return response.json()["response"]
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        import requests
        
        response = requests.post(
            f"{self.base_url}/api/chat",
            json={
                "model": self.model,
                "messages": messages,
                "stream": False,
                **kwargs
            }
        )
        response.raise_for_status()
        return response.json()["message"]["content"]


class OpenAILLM(BaseLLM):
    """
    OpenAI API (ë˜ëŠ” í˜¸í™˜ API)
    í™˜ê²½ë³€ìˆ˜: OPENAI_API_KEY
    """
    
    def __init__(
        self,
        model: str = "gpt-4o-mini",
        api_key: str = None,
        base_url: str = None
    ):
        self.model = model
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url
        self.client = None
        self._initialize()
    
    def _initialize(self):
        try:
            from openai import OpenAI
            
            kwargs = {"api_key": self.api_key}
            if self.base_url:
                kwargs["base_url"] = self.base_url
            
            self.client = OpenAI(**kwargs)
        except ImportError:
            print("âš ï¸ openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print("   pip install openai")
    
    def generate(self, prompt: str, **kwargs) -> str:
        if not self.client:
            return "OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            **kwargs
        )
        return response.choices[0].message.content
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        if not self.client:
            return "OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            **kwargs
        )
        return response.choices[0].message.content


class HuggingFaceLLM(BaseLLM):
    """
    HuggingFace ë¡œì»¬ ëª¨ë¸ (ë¬´ë£Œ)
    ì†Œí˜• ëª¨ë¸ ì‚¬ìš©ìœ¼ë¡œ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
    """
    
    def __init__(self, model_name: str = "google/gemma-2-2b-it"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self._initialized = False
    
    def _initialize(self):
        if self._initialized:
            return
        
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            print(f"ğŸ¤– HuggingFace ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
            print("   (ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None
            )
            self._initialized = True
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def generate(self, prompt: str, max_new_tokens: int = 512, **kwargs) -> str:
        self._initialize()
        
        if not self.model:
            return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        if hasattr(self.model, 'device'):
            inputs = inputs.to(self.model.device)
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            **kwargs
        )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°
        response = response[len(prompt):].strip()
        return response
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        # ì±„íŒ… í˜•ì‹ì„ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
        prompt = ""
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            if role == "system":
                prompt += f"System: {content}\n\n"
            elif role == "user":
                prompt += f"User: {content}\n\n"
            elif role == "assistant":
                prompt += f"Assistant: {content}\n\n"
        
        prompt += "Assistant: "
        return self.generate(prompt, **kwargs)


class GroqLLM(BaseLLM):
    """
    Groq API - ì´ˆê³ ì† ë¬´ë£Œ LLM
    """
    
    def __init__(
        self,
        model: str = "llama-3.1-8b-instant",
        api_key: str = None
    ):
        self.model = model
        self.api_key = api_key or os.getenv("GROQ_API_KEY")
        if not self.api_key:
            raise ValueError("GROQ_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ë¥¼ ì œê³µí•˜ì„¸ìš”.")
        self.client = None
        self._initialize()
    
    def _initialize(self):
        try:
            from groq import Groq
            self.client = Groq(api_key=self.api_key)
            print(f"âœ… Groq ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model}")
        except ImportError:
            print("âš ï¸ groq íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print("   pip install groq")
        except Exception as e:
            print(f"âš ï¸ Groq ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def generate(self, prompt: str, **kwargs) -> str:
        if not self.client:
            return "Groq í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1024,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Groq ì‘ë‹µ ì˜¤ë¥˜: {e}"
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        if not self.client:
            return "Groq í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=1024,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Groq ì‘ë‹µ ì˜¤ë¥˜: {e}"


class GeminiLLM(BaseLLM):
    """
    Google Gemini API
    """
    
    def __init__(
        self,
        model: str = "gemini-2.0-flash-lite",
        api_key: str = None
    ):
        self.model = model
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY ë˜ëŠ” GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ë¥¼ ì œê³µí•˜ì„¸ìš”.")
        self.client = None
        self._initialize()
    
    def _initialize(self):
        try:
            import google.generativeai as genai
            
            genai.configure(api_key=self.api_key)
            self.client = genai.GenerativeModel(self.model)
            print(f"âœ… Gemini ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model}")
            
        except ImportError:
            print("âš ï¸ google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print("   pip install google-generativeai")
        except Exception as e:
            print(f"âš ï¸ Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def generate(self, prompt: str, **kwargs) -> str:
        if not self.client:
            return "Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            response = self.client.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"Gemini ì‘ë‹µ ì˜¤ë¥˜: {e}"
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        if not self.client:
            return "Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            # ë©”ì‹œì§€ë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
            prompt = ""
            for msg in messages:
                role = msg["role"]
                content = msg["content"]
                if role == "system":
                    prompt += f"[ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­]\n{content}\n\n"
                elif role == "user":
                    prompt += f"[ì‚¬ìš©ì]\n{content}\n\n"
                elif role == "assistant":
                    prompt += f"[ì–´ì‹œìŠ¤í„´íŠ¸]\n{content}\n\n"
            
            response = self.client.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"Gemini ì‘ë‹µ ì˜¤ë¥˜: {e}"


class SimpleLLM(BaseLLM):
    """
    ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì‘ë‹µ (LLM ì—†ì´ í…ŒìŠ¤íŠ¸ìš©)
    """
    
    def generate(self, prompt: str, **kwargs) -> str:
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ
        if "ì°¸ê³  ë¬¸ì„œ:" in prompt or "[ë¬¸ì„œ" in prompt:
            # ë¬¸ì„œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ìš”ì•½ í˜•íƒœë¡œ ì‘ë‹µ
            return self._summarize_context(prompt)
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬
        for msg in reversed(messages):
            if msg["role"] == "user":
                return self.generate(msg["content"])
        return "ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    def _summarize_context(self, prompt: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°„ë‹¨ ì‘ë‹µ"""
        # ì§ˆë¬¸ ì¶”ì¶œ
        if "ì§ˆë¬¸:" in prompt:
            question = prompt.split("ì§ˆë¬¸:")[-1].split("\n")[0].strip()
        else:
            question = "ì•Œ ìˆ˜ ì—†ëŠ” ì§ˆë¬¸"
        
        # ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ
        docs = []
        if "[ë¬¸ì„œ" in prompt:
            import re
            doc_matches = re.findall(r'\[ë¬¸ì„œ \d+\][^\[]*', prompt)
            docs = [d.strip() for d in doc_matches]
        
        if docs:
            response = f"'{question}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.\n\n"
            response += "ì°¸ê³  ë¬¸ì„œì— ë”°ë¥´ë©´:\n"
            for i, doc in enumerate(docs[:2], 1):
                # ë¬¸ì„œ ë‚´ìš© ê°„ëµí™”
                content = doc.split('\n', 1)[-1] if '\n' in doc else doc
                content = content[:200] + "..." if len(content) > 200 else content
                response += f"\n{i}. {content}\n"
            return response
        
        return f"'{question}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."


def get_llm(
    provider: str = "simple",
    **kwargs
) -> BaseLLM:
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        provider: LLM ì œê³µì ("simple", "ollama", "openai", "huggingface")
        **kwargs: ì¶”ê°€ ì„¤ì •
        
    Returns:
        LLM ì¸ìŠ¤í„´ìŠ¤
    """
    providers = {
        "simple": SimpleLLM,
        "ollama": OllamaLLM,
        "openai": OpenAILLM,
        "huggingface": HuggingFaceLLM,
        "gemini": GeminiLLM,
        "groq": GroqLLM
    }
    
    if provider not in providers:
        print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” provider: {provider}")
        print(f"   ì‚¬ìš© ê°€ëŠ¥: {list(providers.keys())}")
        provider = "simple"
    
    return providers[provider](**kwargs)


if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸ¤– LLM ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # Simple LLM í…ŒìŠ¤íŠ¸
    print("\nğŸ“ Simple LLM í…ŒìŠ¤íŠ¸:")
    llm = get_llm("simple")
    
    test_prompt = """ì°¸ê³  ë¬¸ì„œ:
[ë¬¸ì„œ 1] (í˜ì´ì§€ 15)
ì‚¬íšŒí™”ëŠ” ê°œì¸ì´ ì‚¬íšŒ êµ¬ì„±ì›ìœ¼ë¡œì„œ í•„ìš”í•œ ì–¸ì–´, ê°€ì¹˜, ê·œë²”ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì´ë‹¤.

ì§ˆë¬¸: ì‚¬íšŒí™”ë€ ë¬´ì—‡ì¸ê°€ìš”?"""
    
    response = llm.generate(test_prompt)
    print(f"\nì‘ë‹µ:\n{response}")
    
    print("\n" + "="*60)
    print("âœ… LLM ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("\nğŸ’¡ ì‹¤ì œ LLM ì‚¬ìš© ì‹œ:")
    print("   - Ollama: ollama pull llama3.2 í›„ get_llm('ollama')")
    print("   - OpenAI: OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ì„¤ì • í›„ get_llm('openai')")
    print("="*60 + "\n")


