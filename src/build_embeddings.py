"""
ì„ë² ë”© ë° ë²¡í„°DB ìƒì„± ëª¨ë“ˆ (STEP 3)
- sentence-transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ì²­í¬ ì„ë² ë”©
- ChromaDBì™€ FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° DB ìƒì„±
- output: data/vector_db/ ì— embedding index ì €ì¥
"""

import os
import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional
import sys
import tempfile
import shutil

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
CHUNKS_DIR = DATA_DIR / "chunks"
VECTOR_DB_DIR = DATA_DIR / "vector_db"

# ì„ë² ë”© ëª¨ë¸ ì„¤ì • (í•œêµ­ì–´ ì§€ì› ë¬´ë£Œ ëª¨ë¸)
EMBEDDING_MODEL = "jhgan/ko-sroberta-multitask"


def load_chunks(chunks_path: str) -> List[Dict]:
    """
    ì²­í¬ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    with open(chunks_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get('chunks', [])


def create_embeddings(texts: List[str], model_name: str = EMBEDDING_MODEL) -> np.ndarray:
    """
    í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    from sentence_transformers import SentenceTransformer
    
    print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
    model = SentenceTransformer(model_name)
    
    print(f"ğŸ”„ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘...")
    embeddings = model.encode(
        texts,
        show_progress_bar=True,
        convert_to_numpy=True
    )
    
    print(f"âœ… ì„ë² ë”© ì™„ë£Œ! ì°¨ì›: {embeddings.shape}")
    return embeddings


def build_chroma_db(
    chunks: List[Dict],
    embeddings: np.ndarray,
    collection_name: str = "social_culture",
    persist_directory: str = None
) -> bool:
    """
    ChromaDBë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        import chromadb
        from chromadb.config import Settings
        
        if persist_directory is None:
            persist_directory = str(VECTOR_DB_DIR / "chroma_db")
        
        # ê¸°ì¡´ í´ë” ì‚­ì œ
        chroma_path = Path(persist_directory)
        if chroma_path.exists():
            shutil.rmtree(str(chroma_path))
            print(f"   ê¸°ì¡´ ChromaDB ì‚­ì œ ì™„ë£Œ")
        
        print(f"ğŸ“¦ ChromaDB ìƒì„± ì¤‘: {persist_directory}")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ìƒˆ ì„¤ì •)
        settings = Settings(
            anonymized_telemetry=False,
            allow_reset=True
        )
        client = chromadb.PersistentClient(path=persist_directory, settings=settings)
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±
        try:
            client.delete_collection(name=collection_name)
        except:
            pass
        
        collection = client.create_collection(
            name=collection_name,
            metadata={"description": "ì‚¬íšŒë¬¸í™” êµê³¼ì„œ ë²¡í„° DB"}
        )
        
        # ë°ì´í„° ì¤€ë¹„
        ids = [f"chunk_{chunk['global_chunk_id']}" for chunk in chunks]
        documents = [chunk['text'] for chunk in chunks]
        metadatas = [
            {
                "chunk_id": int(chunk.get('chunk_id', 0)),
                "global_chunk_id": int(chunk.get('global_chunk_id', 0)),
                "page_number": int(chunk.get('page_number', 0)),
                "char_count": int(chunk.get('char_count', 0))
            }
            for chunk in chunks
        ]
        
        # ë°°ì¹˜ë¡œ ì¶”ê°€
        batch_size = 100
        for i in range(0, len(ids), batch_size):
            end_idx = min(i + batch_size, len(ids))
            collection.add(
                ids=ids[i:end_idx],
                embeddings=embeddings[i:end_idx].tolist(),
                documents=documents[i:end_idx],
                metadatas=metadatas[i:end_idx]
            )
            if (i + batch_size) % 500 == 0:
                print(f"   â³ {end_idx}/{len(ids)} ì²­í¬ ì¶”ê°€ ì™„ë£Œ")
        
        print(f"âœ… ChromaDB ì €ì¥ ì™„ë£Œ!")
        print(f"   - ì»¬ë ‰ì…˜: {collection_name}")
        print(f"   - ì´ ë¬¸ì„œ ìˆ˜: {collection.count()}")
        return True
        
    except Exception as e:
        print(f"âš ï¸ ChromaDB ìƒì„± ì‹¤íŒ¨: {e}")
        return False


def build_faiss_index(
    embeddings: np.ndarray,
    index_path: str = None
) -> bool:
    """
    FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    í•œê¸€ ê²½ë¡œ ë¬¸ì œë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•´ ì„ì‹œ í´ë” ì‚¬ìš©
    """
    try:
        import faiss
        
        if index_path is None:
            index_path = str(VECTOR_DB_DIR / "faiss_index.bin")
        
        print(f"ğŸ“¦ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        # ë²¡í„° ì°¨ì›
        dimension = embeddings.shape[1]
        
        # L2 ê±°ë¦¬ ê¸°ë°˜ ì¸ë±ìŠ¤ ìƒì„±
        index = faiss.IndexFlatL2(dimension)
        
        # ë²¡í„° ì¶”ê°€
        index.add(embeddings.astype('float32'))
        
        # í•œê¸€ ê²½ë¡œ ë¬¸ì œ ìš°íšŒ: ì„ì‹œ íŒŒì¼ì— ì €ì¥ í›„ ë³µì‚¬
        try:
            # ì§ì ‘ ì €ì¥ ì‹œë„
            faiss.write_index(index, index_path)
            print(f"âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ!")
        except:
            # ì„ì‹œ í´ë” ì‚¬ìš©
            with tempfile.NamedTemporaryFile(delete=False, suffix='.bin') as tmp:
                tmp_path = tmp.name
            
            faiss.write_index(index, tmp_path)
            shutil.copy(tmp_path, index_path)
            os.remove(tmp_path)
            print(f"âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ! (ì„ì‹œ ê²½ë¡œ ì‚¬ìš©)")
        
        print(f"   - ê²½ë¡œ: {index_path}")
        print(f"   - ë²¡í„° ìˆ˜: {index.ntotal}")
        print(f"   - ì°¨ì›: {dimension}")
        return True
        
    except Exception as e:
        print(f"âš ï¸ FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return False


def save_embeddings_numpy(
    embeddings: np.ndarray,
    chunks: List[Dict],
    output_dir: str = None
) -> None:
    """
    ì„ë² ë”©ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ NumPy/JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    if output_dir is None:
        output_dir = str(VECTOR_DB_DIR)
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ì„ë² ë”© ì €ì¥
    embeddings_path = output_path / "embeddings.npy"
    np.save(str(embeddings_path), embeddings)
    print(f"ğŸ’¾ ì„ë² ë”© ì €ì¥: {embeddings_path}")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        "total_chunks": len(chunks),
        "embedding_dim": embeddings.shape[1],
        "embedding_model": EMBEDDING_MODEL,
        "chunks_info": [
            {
                "id": i,
                "global_chunk_id": chunk.get('global_chunk_id', i),
                "page_number": chunk.get('page_number', 0),
                "char_count": chunk.get('char_count', 0),
                "text_preview": chunk['text'][:100] + "..." if len(chunk['text']) > 100 else chunk['text']
            }
            for i, chunk in enumerate(chunks)
        ]
    }
    
    metadata_path = output_path / "embeddings_metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")


def process_chunks_file(chunks_path: str, use_faiss: bool = True, use_chroma: bool = True) -> None:
    """
    ì²­í¬ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì„ë² ë”© ë° ë²¡í„° DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    chunks_file = Path(chunks_path)
    
    if not chunks_file.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunks_path}")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {chunks_file.name}")
    print(f"{'='*60}")
    
    # ì²­í¬ ë¡œë“œ
    chunks = load_chunks(str(chunks_file))
    print(f"ğŸ“– {len(chunks)}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    texts = [chunk['text'] for chunk in chunks]
    
    # ì„ë² ë”© ìƒì„±
    embeddings = create_embeddings(texts)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)
    
    # NumPy í˜•ì‹ìœ¼ë¡œ ì €ì¥ (í•­ìƒ)
    save_embeddings_numpy(embeddings, chunks)
    
    # ChromaDB ìƒì„±
    chroma_success = False
    if use_chroma:
        chroma_success = build_chroma_db(chunks, embeddings)
    
    # FAISS ì¸ë±ìŠ¤ ìƒì„±
    faiss_success = False
    if use_faiss:
        faiss_success = build_faiss_index(embeddings)
    
    print(f"\nâœ… {chunks_file.name} ì„ë² ë”© ì™„ë£Œ!")
    print(f"   - NumPy: âœ…")
    print(f"   - ChromaDB: {'âœ…' if chroma_success else 'âŒ'}")
    print(f"   - FAISS: {'âœ…' if faiss_success else 'âŒ'}")


def process_all_chunks() -> None:
    """
    data/chunks/ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ì²­í¬ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    chunks_files = list(CHUNKS_DIR.glob("*_chunks.json"))
    
    if not chunks_files:
        print("âš ï¸ ì²˜ë¦¬í•  ì²­í¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ë¨¼ì € chunking.pyë¥¼ ì‹¤í–‰í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•˜ì„¸ìš”.")
        return
    
    print(f"\nğŸ” ë°œê²¬ëœ ì²­í¬ íŒŒì¼: {len(chunks_files)}ê°œ")
    for f in chunks_files:
        print(f"   - {f.name}")
    
    for chunks_file in chunks_files:
        process_chunks_file(str(chunks_file))
    
    print(f"\nâœ… ëª¨ë“  íŒŒì¼ ì„ë² ë”© ì™„ë£Œ!")


if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸš€ ì„ë² ë”© ë° ë²¡í„°DB ìƒì„± ì‹œì‘")
    print(f"   - ì„ë² ë”© ëª¨ë¸: {EMBEDDING_MODEL}")
    print("="*60)
    
    if len(sys.argv) > 1:
        # íŠ¹ì • íŒŒì¼ ì²˜ë¦¬
        chunks_path = sys.argv[1]
        process_chunks_file(chunks_path)
    else:
        # ëª¨ë“  ì²­í¬ íŒŒì¼ ì²˜ë¦¬
        process_all_chunks()
    
    print("\n" + "="*60)
    print("ğŸ‰ ì„ë² ë”© ë° ë²¡í„°DB ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {VECTOR_DB_DIR}")
    print("="*60 + "\n")
