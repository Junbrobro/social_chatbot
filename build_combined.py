"""ë³¸ë¬¸ + í•´ì„¤ ì²­í¬ë¥¼ í•©ì³ì„œ ë²¡í„°DB ë¹Œë“œ"""
import sys
sys.path.insert(0, 'src')

import json
import numpy as np
from pathlib import Path

DATA_DIR = Path("data")
CHUNKS_DIR = DATA_DIR / "chunks"
VECTOR_DB_DIR = DATA_DIR / "vector_db"

print("="*60)
print("ğŸ”„ ë³¸ë¬¸ + í•´ì„¤ í†µí•© ë²¡í„°DB ë¹Œë“œ")
print("="*60)

# 1. ëª¨ë“  ì²­í¬ ë¡œë“œ ë° í•©ì¹˜ê¸°
all_chunks = []
chunk_files = list(CHUNKS_DIR.glob("*_chunks.json"))

print(f"\nğŸ“– ì²­í¬ íŒŒì¼ ë¡œë“œ ì¤‘...")
for chunk_file in chunk_files:
    print(f"   - {chunk_file.name}")
    with open(chunk_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    chunks = data.get('chunks', [])
    
    # ì†ŒìŠ¤ íŒŒì¼ ì •ë³´ ì¶”ê°€
    source = chunk_file.stem.replace('_chunks', '')
    for chunk in chunks:
        chunk['source_file'] = source
    
    all_chunks.extend(chunks)

print(f"\nâœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ!")

# 2. global_chunk_id ì¬í• ë‹¹
for i, chunk in enumerate(all_chunks):
    chunk['global_chunk_id'] = i

# 3. ì„ë² ë”© ìƒì„±
print(f"\nğŸ¤– ì„ë² ë”© ìƒì„± ì¤‘...")
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("jhgan/ko-sroberta-multitask")
texts = [chunk['text'] for chunk in all_chunks]
embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)

print(f"âœ… ì„ë² ë”© ì™„ë£Œ! shape: {embeddings.shape}")

# 4. NumPy ì €ì¥
VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)
np.save(str(VECTOR_DB_DIR / "embeddings.npy"), embeddings)
print(f"ğŸ’¾ NumPy ì„ë² ë”© ì €ì¥ ì™„ë£Œ")

# 5. ë©”íƒ€ë°ì´í„° ì €ì¥
metadata = {
    "total_chunks": len(all_chunks),
    "embedding_dim": embeddings.shape[1],
    "model": "jhgan/ko-sroberta-multitask",
    "chunks": all_chunks
}
with open(VECTOR_DB_DIR / "embeddings_metadata.json", 'w', encoding='utf-8') as f:
    json.dump(metadata, f, ensure_ascii=False, indent=2)
print(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")

# 6. FAISS ì¸ë±ìŠ¤ ìƒì„±
print(f"\nğŸ“¦ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
import faiss

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings.astype('float32'))
faiss.write_index(index, str(VECTOR_DB_DIR / "faiss_index.bin"))

print(f"âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ!")
print(f"   - ë²¡í„° ìˆ˜: {index.ntotal}")
print(f"   - ì°¨ì›: {dimension}")

# 7. í†µí•© ì²­í¬ íŒŒì¼ ì €ì¥
combined_chunks_path = CHUNKS_DIR / "combined_all_chunks.json"
combined_data = {
    "metadata": {
        "sources": [f.stem for f in chunk_files],
        "total_chunks": len(all_chunks)
    },
    "chunks": all_chunks
}
with open(combined_chunks_path, 'w', encoding='utf-8') as f:
    json.dump(combined_data, f, ensure_ascii=False, indent=2)
print(f"ğŸ’¾ í†µí•© ì²­í¬ íŒŒì¼ ì €ì¥: {combined_chunks_path}")

print("\n" + "="*60)
print("ğŸ‰ í†µí•© ë²¡í„°DB ë¹Œë“œ ì™„ë£Œ!")
print(f"   - ì´ ì²­í¬: {len(all_chunks)}ê°œ")
print(f"   - ë³¸ë¬¸: 1,130ê°œ + í•´ì„¤: 349ê°œ = {len(all_chunks)}ê°œ")
print("="*60)





